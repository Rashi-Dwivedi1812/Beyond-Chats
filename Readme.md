# ğŸš€ BeyondChats â€“ Full Stack Automation Assignment

**Role Applied:** Full Stack Developer Intern  
**Tech Focus:** Web Scraping â€¢ Backend APIs â€¢ Automation â€¢ AI Integration â€¢ React UI

---

## ğŸ‘‹ Overview (For Recruiters)

This project is a **production-style full stack automation system** built as part of a Full Stack Development internship assignment.

It demonstrates my ability to:
- Design clean backend APIs
- Scrape and process real-world web data
- Build reliable automation pipelines
- Integrate AI services responsibly (with fallbacks)
- Present data clearly using a modern React UI

The system **scrapes original blog articles**, enhances them using **external references and AI**, and displays **both original and updated versions** without overwriting data.

---

## ğŸ§  What This Project Does (High-Level)

1. Scrapes original blog articles from **BeyondChats**
2. Stores them in **MongoDB**
3. Searches Google for related high-ranking articles
4. Scrapes content from external sources
5. Uses an LLM to enhance the original article
6. Publishes a **new AI-updated article**
7. Displays both versions in a **React frontend**

---

## âœ… Key Engineering Highlights

- **Web Scraping:** Axios + Cheerio
- **REST APIs:** Node.js, Express, MongoDB
- **Automation Pipeline:** Robust, error-tolerant design
- **AI Integration:** LLM-based rewriting with graceful fallback
- **Data Integrity:** Originals are never overwritten
- **Frontend:** Clean, responsive React UI
- **Security:** Environment variables, no secrets committed

---

## ğŸ—ï¸ Tech Stack & Rationale

### Frontend
- React.js â€“ Component-based architecture for building a clean, scalable UI
- Tailwind CSS â€“ Rapid, consistent styling with responsive design support
- Axios â€“ Reliable HTTP client for frontendâ€“backend communication

### Backend
- Node.js â€“ Event-driven runtime suitable for I/O-heavy operations like scraping and APIs
- Express.js â€“ Lightweight framework for building structured REST APIs
- MongoDB â€“ Flexible NoSQL database ideal for handling evolving article schemas
- Mongoose â€“ Schema modeling and validation for better data consistency

### Automation & Data Processing
- Axios â€“ HTTP requests for scraping and API communication
- Cheerio â€“ Fast HTML parsing for extracting structured content from blogs
- Google Search API â€“ Fetching high-ranking external reference articles

### AI Integration
- OpenAI API (LLM) â€“ Used to enhance and rewrite content based on reference articles
- Fallback Handling â€“ Ensures the automation pipeline continues even if AI services fail

---

## ğŸ“‚ Project Structure
```bash
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ controllers/
â”‚ â”‚ â”œâ”€â”€ routes/
â”‚ â”‚ â”œâ”€â”€ models/
â”‚ â”‚ â””â”€â”€ services/
â”‚ â””â”€â”€ server.js
â”‚
â”œâ”€â”€ automation/
â”‚ â”œâ”€â”€ index.js
â”‚ â”œâ”€â”€ googleSearch.js
â”‚ â”œâ”€â”€ scrapeArticle.js
â”‚ â”œâ”€â”€ publishArticle.js
â”‚ â””â”€â”€ rewriteWithLLM.js
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ src/
â”‚
â””â”€â”€ README.md
```

---

## ğŸ—„ï¸ Database Design

### Original Article
```json
{
  "title": "Article Title",
  "content": "Scraped content",
  "isOriginal": true,
  "isUpdated": false
}
```

### AI-Updated Article
```json
{
  "title": "Article Title",
  "content": "AI-enhanced content",
  "references": ["external_link_1", "external_link_2"],
  "isOriginal": false,
  "isUpdated": true,
  "originalArticleId": "ObjectId"
}
```

## ğŸ”„ Automation Workflow (Phase 2)
- The automation script performs the following steps:
- Fetches original articles from backend API
- Searches the article title on Google
- Selects top external blog/article links
- Scrapes meaningful reference content
- Attempts AI-based rewriting using an LLM
- Applies a safe fallback if AI fails
- Publishes the AI-updated article via backend API

This workflow is designed to continue execution even if external services (e.g., LLM APIs) fail.


## ğŸ–¥ Frontend (Phase 3)
- The React frontend:
- Fetches articles from backend APIs
- Clearly labels Original and Updated articles
- Displays reference links for updated articles
- Uses a clean, responsive, professional UI


## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Backend
```bash
cd backend
npm install
npm start
```

### 2ï¸âƒ£ Scrape Original Articles
```bash
curl -X POST http://localhost:5000/api/articles/scrape
```

### 3ï¸âƒ£ Run Automation
```bash
cd automation
node index.js
```

### 4ï¸âƒ£ Frontend
```bash
cd frontend
npm install
npm start
```

### ğŸ” Environment Variables
Create .env files locally (not committed).

Backend .env
```bash
MONGO_URI=mongodb://127.0.0.1:27017/beyondchats
PORT=5000
```

Automation .env
```bash
API_BASE_URL=http://localhost:5000/api/articles
OPENAI_API_KEY=your_openai_key
SERP_API_KEY=your_google_search_key
```
--- 

## ğŸ“Œ Final Note
This project focuses on engineering quality, robustness, and clarity, closely aligning with real-world full stack development workflows used in production systems.

---

Thank you for reviewing my submission.

